根据[小林 Coding](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzUxODAzNDg4NQ==&action=getalbum&album_id=1790401816640225283#wechat_redirect) 整理的 Redis 八股，主要是起到大纲的作用。

## 一、缓存异常

> reference: 
>
> * [再也不怕，缓存雪崩、击穿、穿透！](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247490008&idx=1&sn=8f576e69ec63e02a8b42a00ae6754f0a&chksm=f98e5d72cef9d464710c891c4c0537c20e4949b39ee70c97c44c3f6f95df83fc406f52fc161b&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

缓存雪崩、缓存击穿、缓存穿透

* 概念
* 解决方案

## 二、Redis 持久化

> reference: 
>
> * [宕机了，缓存数据没了。。。](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247493808&idx=1&sn=588d318ec6e72844841d566f16acaf30&chksm=f98dac1acefa250ce40060899a4030bb7678c45befea408162360db4ef4fa2785feb1544e719&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)
> * [咔擦，不就是快照嘛](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247494677&idx=1&sn=53f60870b66c731aa6ec5b6e70697eff&chksm=f98da8bfcefa21a9ad7b4d564238931f8457bafb08698301e9672dbc366a034890429fdc1ff1&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

Redis 虽然是内存数据库，但是也可以持久化存储：

* AOF（Append only File）
* RDB（Redis DataBase）

## 三、单线程

> reference：
>
> * [快问快答！](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247495027&idx=1&sn=217af306b07ed0f2a064773541d46721&chksm=f98da9d9cefa20cf7ce2ef0ebf60cf378b79958a2e34559c9da7922be4a946a800ff815f950f&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

Redis 确实是以单线程架构被大家所知，但是这个单线程指的是「**从网络 IO 处理到实际的读写命令处理**」都是由单个线程完成的，并不是说整个 Redis 里只有一个主线程。

有些命令操作可以用后台子进程执行（比如快照生成、AOF 重写）。

严格意义上说的话，Redis 4.0 之后并不是单线程架构了，除了主线程外，它也有后台线程在处理一些耗时比较长的操作，例如清理脏数据、无用连接的释放、大 Key 的删除等等。

你可能听到 Redis 6.0 版本支持了多线程技术，不过这个并不是指多个线程同时在处理读写命令，而是使用多线程来处理 Socket 的读写，**最终执行读写命令的过程还是只在主线程里**。

之所以采用多线程 IO 是因为Redis 处理请求时，网络处理经常是瓶颈，通过多个 IO 线程并行处理网络操作，可以提升整体处理性能。

那为什么处理操作命令的过程只在单线程里呢？

因为 Redis 不存在 CPU 成为瓶颈的情况，**主要受限于内存和网络**。

而且使用单线程的好处在于，可维护性高、实现简单。

如果采用多线程模型来处理读写命令，虽然能提升并发性能，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。

## 四、主从复制

> reference：
>
> * [小林差点崩溃了，还好有主从复制](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247495237&idx=1&sn=8302585810958567a57bda3df6c5d036&chksm=f98daaefcefa23f95e9f4ef083bbf6b8ce80794bb00c68d9070f158e6de9a0265ea39b12e560&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

主从复制模式可以保证多态服务器的数据一致性，且主从服务器之间采用的是 **读写分离** 的方式。

主从复制的三种写命令传播模式：

1. 全量复制
2. 基于长 TCO 连接的命令传播
3. 增量复制

主从服务器第一次同步的时候，就是采用全量复制，此时主服务器会两个耗时的地方，分别是生成 RDB 文件和传输 RDB 文件。为了避免过多的从服务器和主服务器进行全量复制，可以把一部分从服务器升级为「经理角色」，让它也有自己的从服务器，通过这样可以分摊主服务器的压力。

第一次同步完成后，主从服务器都会维护着一个长连接，主服务器在接收到写操作命令后，就会通过这个连接将写命令传播给从服务器，来保证主从服务器的数据一致性。

如果遇到网络断开，增量复制就可以上场了，不过这个还跟 `repl_backlog_size `这个大小有关系。

如果它配置的过小，主从服务器网络恢复时，可能发生「从服务器」想读的数据已经被覆盖了，那么这时就会导致主服务器采用全量复制的方式。所以为了避免这种情况的频繁发生，要调大这个参数的值，以降低主从服务器断开后全量同步的概率。

## 五、三种缓存更新策略

> reference：
>
> * [面试官：3 种缓存更新策略是怎样的？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247516371&idx=2&sn=1976ef550b5b0a1d52f7ca09ec9d5d80&chksm=f98dc479cefa4d6fd74412c99f12b21ed3cee4b8e74c005710ce5bc575d0832e96e6184126c5&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

对于每一种策略，在读模式或写模式下是怎样的？在读模式下，主要考虑的是数据不在缓存时，由谁加载数据？

1.Cache Aside（旁路缓存）策略

* 应用程序与缓存和数据库交互

* Cache Aside 需要先写入数据库，再删除缓存，而不能先删除缓存，再更新数据库

* 对于先删除缓存，再写入数据库导致的数据不一致，可以使用延迟双删策略：

  ``` shell
  #删除缓存
  redis.delKey(X)
  #更新数据库
  db.update(X)
  #睡眠
  Thread.sleep(N)
  #再删除缓存
  redis.delKey(X)
  ```

  加了个睡眠时间，主要是为了确保请求 A 在睡眠的时候，请求 B 能够在这这一段时间完成「从数据库读取数据，再把缺失的缓存写入缓存」的操作，然后请求 A 睡眠完，再删除缓存。

  所以，请求 A 的睡眠时间就需要大于请求 B 「从数据库读取数据 + 写入缓存」的时间。

  但是具体睡眠多久其实是个**玄学**，很难评估出来，所以这个方案也只是**尽可能**保证一致性而已，极端情况下，依然也会出现缓存不一致的现象。

  因此，还是比较建议用「先更新数据库，再删除缓存」的方案。

2.Read/Write Through（读穿/写穿）策略

* 应用程序与缓存交互，缓存和数据库交互
* 搭配写不分配
* 相较于 Cache Aside 比较少见，因为例如 Redis 等内存数据库都不提供写入数据库和自动加载数据库中的数据的能力

3.Write Back（写回）策略

* 应用程序与缓存交互，缓存和数据库交互
* 搭配写分配
* 对于脏数据的刷盘，缓存会通过批量异步更新的方式进行
* 和 Write Through 一样，Write Back 也很少应用到数据库和缓存的场景中，因为 Redis 没有异步更新数据库的能力

## 六、数据库和缓存如何保持一致性

> reference:
>
> * [老板真爱画大饼！](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247497782&idx=1&sn=89fd676bbe63a819ad1e32976a0755ee&chksm=f98dbc9ccefa358ab0379e4e7799f53668cb31d401ed329658d05e9aea83949a6898b46279e5&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)
> * [趣说 ｜ 数据库和缓存如何保证一致性？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247508038&idx=2&sn=50a5d4a4b88f9572976753562aeb1b52&chksm=f98de4eccefa6dfaf9c48451685b51a7be62f237cb5ab13efb75a83b814437a370b2f8abfa88&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

### 1. 先更新数据库，还是先更新缓存？

注意这里是更新缓存而不是删除缓存，不同于 Cache Aside，这里无论是先更新数据库，还是先更新缓存，都会出现并发问题。

* 考虑两条并发执行的指令，分别将 val 的值设置为 1,2，由于指令的交叉执行，无论先更新数据库还是先更新缓存，都会出现并发问题。

如何解决该问题，参考 Cache Aside，修改更新缓存操作为删除缓存，就可以保证数据的一致性。

此外，为了万无一失，还可以给数据加上过期时间，这样即使缓存数据不一致，也有过期时间来兜底，不会造成太大影响。

不过，虽然 Cache Aside 有效解决了一致性问题，但是它粗暴的删除缓存会导致缓存命中率下降，特别是业务是写多读少的情况，对性能的影响比较大。

如果我们不能接受缓存的频繁 miss，只能是更新缓存而不是删除缓存，此时为了保证数据库和缓存的一致性，有以下两种解决方案：

1. 在更新缓存前加个分布式锁，但是锁会影响 redis 的访问。
2. 在更新完缓存之后，加上一个较短的过期时间，降低数据不一致时的损失。

### 2. 删除缓存的时候失败了

在先更新数据库再删除缓存的方案中，如果在删除缓存的时候失败了，导致缓存中的数据依然是旧值怎么办？

有以下两种方案：

* 重试机制

  我们可以引入**消息队列**，将第二个操作（删除缓存）要操作的数据加入到消息队列，由消费者来操作数据。

  - 如果应用**删除缓存失败**，可以从消息队列中重新读取数据，然后再次删除缓存，这个就是**重试机制**。当然，如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了。
  - 如果**删除缓存成功**，就要把数据从消息队列中移除，避免重复操作，否则就继续重试。

* 订阅 Mysql binlog，再操作缓存

  「**先更新数据库，再删缓存**」的策略的第一步是更新数据库，那么更新数据库成功，就会产生一条变更日志，记录在 binlog 里。

  于是我们就可以通过订阅 binlog 日志，拿到具体要操作的数据，然后再执行缓存删除，阿里巴巴开源的 Canal 中间件就是基于这个实现的。

所以，如果要想保证「先更新数据库，再删缓存」策略第二个操作能执行成功，我们可以使用「消息队列来重试缓存的删除」，或者「订阅 MySQL binlog 再操作缓存」，这两种方法有一个共同的特点，都是采用 **异步操作** 缓存。

## 七、过期删除策略

每当我们对一个 key 设置了过期时间时，Redis  会把该 key 带上过期时间存储到一个**过期字典**（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。

当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中：

- 如果不在，则正常读取键值；
- 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。

常见的三种过期删除策略：

- 定时删除：cpu 不友好，内存友好
- 惰性删除：cpu 友好，内存不友好
- 定期删除：折中吗。**每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。**

前面介绍了三种过期删除策略，每一种都有优缺点，仅使用某一个策略都不能满足实际需求。

所以， **Redis 选择「惰性删除+定期删除」这两种策略配和使用**，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。

## 八、内存淘汰策略

前面说的过期删除策略，是删除已过期的 key，而当 Redis 的运行内存已经超过 Redis 设置的最大内存之后，则会使用内存淘汰策略删除符合条件的 key，以此来保障 Redis 高效的运行。

在配置文件 `redis.conf` 中，可以通过参数 `maxmemory <bytes>` 来设定最大运行内存，只有在 Redis 的运行内存达到了我们设置的最大运行内存，才会触发内存淘汰策略。

不同位数的操作系统，maxmemory 的默认值是不同的：

- 在 64 位操作系统中，maxmemory 的默认值是 0，表示没有内存大小限制，那么不管用户存放多少数据到 Redis 中，Redis 也不会对可用内存进行检查，直到 Redis 实例因内存不足而崩溃也无作为。
- 在 32 位操作系统中，maxmemory 的默认值是 3G，因为 32 位的机器最大只支持 4GB 的内存，而系统本身就需要一定的内存资源来支持运行，所以 32 位操作系统限制最大 3 GB 的可用内存是非常合理的，这样可以避免因为内存不足而导致 Redis 实例崩溃。

Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。其中，「进行数据淘汰」又分为 「只淘汰过期数据」和「可以淘汰任意数据」。

***1、不进行数据淘汰的策略***

* **noeviction**（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。

***2、进行数据淘汰的策略***

针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。

***2.1、在设置了过期时间的数据中进行淘汰：***

- **volatile-random**：随机淘汰设置了过期时间的任意键值；
- **volatile-ttl**：优先淘汰更早过期的键值。
- **volatile-lru**（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值；
- **volatile-lfu**（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用频率的键值；

***2.2、在所有数据范围内进行淘汰：***

- **allkeys-random**：随机淘汰任意键值;
- **allkeys-lru**：淘汰整个键值中最久未使用的键值；
- **allkeys-lfu**（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。

---

值得注意的是，Redis 实现的并不是传统的基于链表的 LRU，因为传统的 LRU 算法存在两个问题：

1. 需要用链表管理所有的缓存数据，这会带来额外的空间开销；
2. 当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。

Redis 实现的是一种**近似 LRU 算法**，目的是为了更好的节约内存，它的**实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间**。

当 Redis 进行内存淘汰时，会使用**随机采样的方式来淘汰数据**，它是随机取 5 个值（此值可配置），然后**淘汰最久没有使用的那个**。

Redis 实现的 LRU 算法的优点：

- 不用为所有的数据维护一个大链表，节省了空间占用；
- 不用在每次数据访问时都移动链表项，提升了缓存的性能；

但是 LRU 算法有一个问题，**无法解决缓存污染问题**，比如应用一次读取了大量的数据，而这些数据只会被读取这一次，那么这些数据会留存在 Redis 缓存中很长一段时间，造成缓存污染。

因此，在 Redis 4.0 之后引入了 LFU 算法来解决这个问题。

LFU 全称是 Least Frequently Used 翻译为**最近最不常用的，**LFU 算法是根据数据访问次数来淘汰数据的，它的核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。

所以， LFU 算法会记录每个数据的访问次数。当一个数据被再次访问时，就会增加该数据的访问次数。这样就解决了偶尔被访问一次之后，数据留存在缓存中很长一段时间的问题，相比于 LRU 算法也更合理一些。

LFU 算法相比于  LRU 算法的实现，多记录了「数据的访问频次」的信息。

Redis 对象的结构如下：

```cpp
typedef struct redisObject {
    ...
      
    // 24 bits，用于记录对象的访问信息
    unsigned lru:24;  
    ...
} robj;
```

Redis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。

在 LRU 算法中，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。

在 LFU 算法中，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，低 8bit 存储 logc(Logistic Counter)。

- ldt 是用来记录 key 的访问时间戳；
- logc 是用来记录 key 的访问频次，它的值越小表示使用频率越低，越容易淘汰，每个新加入的 key 的logc 初始值为 5。

注意，logc 并不是单纯的访问次数，而是访问频次（访问频率），因为 **logc  会随时间推移而衰减的**。

在每次 key 被访问时，会先对 logc 做一个衰减操作，衰减的值跟前后访问时间的差距有关系，如果上一次访问的时间与这一次访问的时间差距很大，那么衰减的值就越大，这样实现的 LFU 算法是根据**访问频率**来淘汰数据的，而不只是访问次数。访问频率需要考虑 key 的访问是多长时间段内发生的。key 的先前访问距离当前时间越长，那么这个 key 的访问频率相应地也就会降低，这样被淘汰的概率也会更大。

对 logc 做完衰减操作后，就开始对 logc  进行增加操作，增加操作并不是单纯直接 + 1，而是根据概率增加，如果 logc 越大的 key，它的 logc 就越难再增加。

所以，Redis 在访问 key 时，对于 logc  是这样变化的：

1. 先按照上次访问距离当前的时长，来对 logc 进行衰减；
2. 然后，再按照一定概率增加 logc 的值

redis.conf 提供了两个配置项，用于调整 LFU 算法从而控制 logc 的增长和衰减：

- `lfu-decay-time` 用于调整 logc 的衰减速度，它是一个以分钟为单位的数值，默认值为1，lfu-decay-time 值越大，衰减越慢；
- `lfu-log-factor` 用于调整 logc 的增长速度，lfu-log-factor 值越大，logc 增长越慢。

## 九、你确定 Redis 是单线程的进程吗

> reference:
>
> * [为什么单线程的 Redis 如何做到每秒数万 QPS ？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247511819&idx=2&sn=b6c533071625d9f11c019f8a6c0f1b63&scene=21#wechat_redirect)
> * [面试官：你确定 Redis 是单线程的进程吗？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247516338&idx=2&sn=481c0c5ba605eddbc4824056d941a261&chksm=f98dc418cefa4d0ec4e8e3e2a1c1badc6cb66b5c20775ff785b8ef0da708cf8eb0a1c42c236f&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

### 1. Redis 是单线程吗

**Redis 单线程指的是「接收客户端请求->解析请求 ->进行数据读写等操作->发生数据给客户端」这个过程是由一个线程（主线程）来完成的**，这也是我们常说 Redis 是单线程的原因。

这种单线程模型，结合高效的  **I/O 多路复用机制**（如 epoll/kqueue），使得 Redis 能够避免多线程带来的上下文切换和锁竞争开销，尤其对于其内存中的数据结构操作，能实现非常高的性能。

但是，**Redis 程序并不是单线程的**，Redis 在启动的时候，是会**启动后台线程（BIO）**的：

- **Redis 在 2.6 版本**，会启动 2 个后台线程，分别处理关闭文件、AOF 刷盘这两个任务；
- **Redis 在 4.0 版本之后**，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程。例如执行 unlink key / flushdb async / flushall async 等命令，会把这些删除操作交给后台线程来执行，好处是不会导致 Redis 主线程卡顿。因此，当我们要删除一个大 key 的时候，不要使用 del 命令删除，因为 del 是在主线程处理的，这样会导致 Redis 主线程卡顿，因此我们应该使用 unlink 命令来异步删除大key。

之所以 Redis 为「关闭文件、AOF 刷盘、释放内存」这些任务创建单独的线程来处理，是因为这些任务的操作都是很耗时的，如果把这些任务都放在主线程来处理，那么 Redis 主线程就很容易发生阻塞，这样就无法处理后续的请求了。

后台线程相当于一个消费者，生产者把耗时任务丢到任务队列中，消费者（BIO）不停轮询这个队列，拿出任务就去执行对应的方法即可。

关闭文件、AOF 刷盘、释放内存这三个任务都有各自的任务队列：

- BIO_CLOSE_FILE，关闭文件任务队列：当队列有任务后，后台线程会调用 close(fd) ，将文件关闭；
- BIO_AOF_FSYNC，AOF刷盘任务队列：当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到队列中。当发现队列有任务后，后台线程会调用 fsync(fd)，将 AOF 文件刷盘，
- BIO_LAZY_FREE，lazy free 任务队列：当队列有任务后，后台线程会 free(obj) 释放对象 / free(dict) 删除数据库所有对象 / free(skiplist) 释放跳表对象；

### 2. Redis 单线程模式是怎样的

Redis 6.0 版本之前的单线模式如下图：

![redis_single_thread_before_v6.0](https://github.com/QaQOwOQaQ/picx-images-hosting/raw/master/image.8adihgi3ru.png)

### 3. Redis 采用单线程为什么还这么快

官方使用基准测试的结果是，**单线程的 Redis 吞吐量可以达到 10W/每秒**。

之所以 Redis 采用单线程（网络 I/O 和执行命令）那么快，有如下几个原因：

- Redis 的大部分操作**都在内存中完成**，并且采用了高效的数据结构，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU，既然 CPU 不是瓶颈，那么自然就采用单线程的解决方案了；
- Redis 采用单线程模型可以**避免了多线程之间的竞争**，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。
- Redis 采用了 **I/O 多路复用机制**处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。

### 4. Redis 6.0 之前为什么使用单线程

我们都知道单线程的程序是无法利用服务器的多核 CPU 的，那么早期 Redis 版本的主要工作（网络 I/O 和执行命令）为什么还要使用单线程呢？我们不妨先看一下Redis官方给出的FAQ。

![why_single_thread_before_v6.0](https://github.com/QaQOwOQaQ/picx-images-hosting/raw/master/image.51eekt0lh8.webp)

核心意思是：**CPU 并不是制约 Redis 性能表现的瓶颈所在**，更多情况下是受到内存大小和网络I/O的限制，所以 Redis 核心网络模型使用单线程并没有什么问题，如果你想要使用服务的多核CPU，可以在一台服务器上启动多个节点或者采用分片集群的方式。

除了上面的官方回答，选择单线程的原因也有下面的考虑。

使用了单线程后，可维护性高，多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，**增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗**。

### 5. Redis 6.0 之后为什么引入了多线程?

虽然 Redis 的主要工作（网络 I/O 和执行命令）一直是单线程模型，但是在 Redis 6.0 版本之后，也采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在 **网络 I/O 的处理** 上。

> 在 Redis 的语境中，**网络 I/O (Input/Output)** 主要指以下两个核心环节：
>
> 1. **从客户端读取数据 (Input)：**
>    - **接收原始字节流：** 当客户端向 Redis 发送命令时，数据通过网络传输到达 Redis 服务器。这涉及底层的 TCP/IP 协议栈，Redis 需要从操作系统的网络缓冲区中读取这些原始字节。
>    - **协议解析：** 读取到的原始字节流是按照 Redis 自己的通信协议 (RESP, Redis Serialization Protocol) 进行编码的。Redis 需要解析这些字节流，将其还原成客户端发送的命令（例如 `SET key value`、`GET key` 等），并提取出命令参数。
> 2. **向客户端写入数据 (Output)：**
>    - **构建响应：** 在 Redis 主线程执行完命令（例如 `GET key` 后获取到 `value`）之后，它需要将结果按照 RESP 协议编码成字节流。
>    - **发送数据到网络：** 编码好的字节流需要通过网络发送回客户端。这同样涉及底层的 TCP/IP 协议栈，Redis 需要将这些数据写入操作系统的网络缓冲区，最终通过网络传输到客户端。
>
> **在 Redis 6.0 之前，这两个过程（读取、解析、构建响应、写入）都是由唯一的“主线程”来完成的。** 尽管 Redis 使用了 **I/O 多路复用** (如 epoll/kqueue) 技术来高效地监听多个客户端连接的读写事件，避免了阻塞，但当某个连接有数据可读或可写时，具体的读写和协议解析工作仍然由主线程亲自完成。
>
> **Redis 6.0 引入多线程后，这里的“网络 I/O”具体是指将上述步骤中的：**
>
> - **从客户端读取原始字节流**
> - **对读取到的原始字节流进行协议解析**
> - **将执行结果编码成字节流**
> - **将编码后的字节流写入网络**
>
> 这部分**耗时的网络数据处理和协议编解码工作**，从主线程中剥离出来，交由多个 **I/O 线程**（通常配置为 `io-threads` 数量）来并行处理。
>
> **职责划分大致如下：**
>
> - **主线程 (Main Thread):**
>   - 负责监听客户端连接的接入（`accept`）。
>   - **核心命令的执行：** 所有的 Redis 命令（如 `GET`、`SET`、`LPUSH` 等）仍然由主线程串行执行，这保证了 Redis 单命令操作的原子性。
>   - 将解析好的命令放入执行队列。
>   - 从 I/O 线程获取命令执行结果，并准备待写入的数据。
> - **I/O 线程 (IO Threads):**
>   - **并行读取客户端数据：** 从网络 socket 中读取原始字节流。
>   - **并行解析客户端命令：** 将读取到的字节流按照 RESP 协议解析成 Redis 命令结构。
>   - **并行编码响应数据：** 将主线程执行后的结果，编码成 RESP 协议的字节流。
>   - **并行写入响应数据：** 将编码好的字节流写入到网络 socket，发送回客户端。
>
> 通过这种方式，Redis 6.0 的多线程 I/O 模块有效地将网络 I/O 的阻塞和计算密集型任务（协议解析和编码）分散到多个 CPU 核上，从而减少了主线程在这些任务上的等待时间，显著提升了在高并发网络流量下的吞吐量。

所以为了提高网络请求处理的并行度，Redis 6.0 对于网络请求采用多线程来处理。**但是对于命令执行，Redis 仍然使用单线程来处理，**所以大家**不要误解** Redis 有多线程同时执行命令。

Redis 官方表示，**Redis 6.0 版本引入的多线程 I/O 特性对性能提升至少是一倍以上**。

Redis 6.0 版本支持的 I/O  多线程特性，默认是 I/O 多线程只处理写操作（write client socket），并不会以多线程的方式处理读操作（read client socket）。要想开启多线程处理客户端读请求，就需要把  Redis.conf  配置文件中的 io-threads-do-reads 配置项设为 yes。

```
//读请求也使用io多线程
io-threads-do-reads yes 
```

同时， Redis.conf  配置文件中提供了  IO 多线程个数的配置项。

```
// io-threads N，表示启用 N-1 个 I/O 多线程（主线程也算一个 I/O 线程）
io-threads 4 
```

关于线程数的设置，官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好。因此， **Redis 6.0 版本之后，**Redis 在启动的时候，默认情况下会有 6 个线程：

- Redis-server ：Redis的主线程，主要负责执行命令；
- bio_close_file、bio_aof_fsync、bio_lazy_free：三个后台线程，分别异步处理关闭文件任务、AOF刷盘任务、释放内存任务；
- io_thd_1、io_thd_2、io_thd_3：三个 I/O 线程，io-threads 默认是 4 ，所以会启动 3（4-1）个 I/O 多线程，用来分担 Redis 网络 I/O 的压力。

## 十、用 Redis 实现分布式锁

> reference:
>
> * [面试官：如何用 Redis 实现分布式锁？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247517757&idx=2&sn=67d56ebc0accd449c75ec6c36fca8645&chksm=f98dc297cefa4b811a2cf04d60b5f28f8e2d59e1281cf3e433579bb164767342b0499f6f689a&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)
> * [Is Redlock Safe? 一场关于 Redlock 的辩论](https://juejin.cn/post/7049588479025479717)
> * [Distributed Locks with Redis](https://redis.io/docs/latest/develop/use/patterns/distributed-locks/)

### 1. 为什么 Redis 可以实现分布式锁

分布式锁是用于分布式环境下并发控制的一种机制，用于控制某个资源在同一时刻只能被一个应用所使用。

Redis 本身可以被多个客户端共享访问，正好就是一个共享存储系统，可以用来保存分布式锁，而且 Redis 的读写性能高，可以应对高并发的锁操作场景。

Redis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁：

- 如果 key 不存在，则显示插入成功，可以用来表示加锁成功；
- 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。

基于 Redis 节点实现分布式锁时，对于加锁操作，我们需要满足三个条件。

- 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；
- 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间；
- 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端；

满足这三个条件的分布式命令如下：

```shell
SET lock_key unique_value NX PX 10000 
```

- lock_key 就是 key 键；
- unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作；
- NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；
- PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。

而解锁的过程就是将 lock_key 键删除（del lock_key），但不能乱删，要保证执行操作的客户端就是加锁的客户端。所以，解锁的时候，我们要先判断锁的 unique_value 是否为加锁客户端，是的话，才将 lock_key 键删除。

可以看到，解锁是有两个操作，这时就需要 **Lua 脚本** 来保证解锁的原子性，因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，保证了锁释放操作的原子性。

```lua
// 释放锁时，先比较 unique_value 是否相等，避免锁的误释放
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

这样一来，就通过使用 SET 命令和 Lua 脚本在 Redis 单节点上完成了分布式锁的加锁和解锁。

### 2. 基于 Redis 实现的分布式锁有什么优缺点

基于 Redis 实现分布式锁的**优点**：

1. 性能高效（这是选择缓存实现分布式锁最核心的出发点，Redis 是缓存数据库，对 key 的访问很高效）。
2. 实现方便。很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。
3. 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。

基于 Redis 实现分布式锁的**缺点**：

- **超时时间不好设置**。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效，注意 A 线程没执行完，后续线程 B 又意外的持有了锁，意味着可以操作共享资源，那么两个线程之间的共享资源就没办法进行保护了。

- - **那么如何合理设置超时时间呢？** 我们可以基于 **续约** 的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂。

- **Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性**。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。

### 3. 如何解决集群情况下 Redis 分布式锁的可靠性

为了保证集群环境下分布式锁的可靠性，Redis 官方已经设计了一个分布式锁算法 Redlock（红锁）。

它是基于多个 Redis 节点的分布式锁，即使有节点发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。

Redlock 算法的基本思路，**是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败**。

这样一来，即使有某个 Redis 节点发生故障，因为锁的数据在其他节点上也有保存，所以客户端仍然可以正常地进行锁操作，锁的数据也不会丢失。

Redlock 算法加锁三个过程：

- 第一步是，客户端获取当前时间。

- 第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作：

- - 加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。
  - 如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给「加锁操作」设置一个超时时间（不是对「锁」设置超时时间，而是对「加锁操作」设置超时时间）。

- 第三步是，一旦客户端完成了和所有 Redis 节点的加锁操作，客户端就要计算整个加锁过程的总耗时（t1）。

加锁成功要同时满足两个条件（*简述：如果有超过半数的 Redis 节点成功的获取到了锁，并且总耗时没有超过锁的有效时间，那么就是加锁成功*）：

- 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁；
- 条件二：客户端获取锁的总耗时（t1）没有超过锁的有效时间。

加锁成功后，客户端需要重新计算这把锁的有效时间，计算的结果是「锁的最初有效时间」减去「客户端为获取锁的总耗时（t1）」。

加锁失败后，客户端向所有 Redis 节点发起释放锁的操作，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。

## 十一、为什么 Redis 要有哨兵

> reference:
>
> * [面试官：为什么 Redis 要有哨兵？监控，选主，通知](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247515688&idx=2&sn=9680569edd01c327253c7eb87a75ab94&chksm=f98dfa82cefa73943fa320a1debda7c7d461c675a41cbf4c9808009f6c082588ab194344f4f0&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

### 1. 什么是哨兵

在 Redis 中，**哨兵 (Sentinel)** 是一个用于实现 **高可用性 (High Availability)** 的分布式系统。它不是一个数据存储服务，而是一个特殊的 Redis 进程，主要职责是监控 Redis 主从集群的健康状况，并在主节点发生故障时自动进行故障转移，从而确保 Redis 服务的持续可用性。

**哨兵的主要作用和概念总结如下：**

1. **监控 (Monitoring)**：
   - 哨兵会持续地监控 Redis 主节点和从节点（以及其他哨兵节点）的运行状态。
   - 它会定期发送 PING 命令，检查各个节点是否在线、响应是否正常。
   - 如果发现某个节点长时间没有响应，哨兵会将其标记为下线。
2. **故障检测与通知 (Failure Detection and Notification)**：
   - 当一个主节点被某个哨兵判断为下线时，它会向其他哨兵节点询问，看其他哨兵是否也认为该主节点下线。
   - 如果达到一定数量（**法定数量 Quorum**）的哨兵都认为主节点下线了，那么主节点就被正式标记为“客观下线” (Objectively Down)。
   - 一旦主节点被标记为客观下线，哨兵可以向系统管理员或其他应用程序发送通知（例如通过 API）。
3. **自动故障转移 (Automatic Failover)**：
   - 当主节点客观下线后，哨兵集群会进行一次 **领头哨兵选举 (Leader Election)**，选出一个哨兵来负责故障转移。
   - 被选中的领头哨兵会在现有从节点中，根据一定的规则（如数据完整性、复制偏移量等）选择一个最合适的从节点，将其提升为新的主节点。
   - 然后，它会通知其他从节点，让它们去复制新的主节点的数据。
   - 最后，它会通知客户端，告知新的主节点地址，以便客户端能够连接到新的主节点继续操作，从而实现服务的无缝切换，最大限度地减少停机时间。
4. **配置提供者 (Configuration Provider)**：
   - 客户端不需要直接知道主节点的地址。它们可以连接到哨兵节点，向哨兵查询当前哪个节点是主节点。
   - 当发生故障转移时，哨兵会自动更新内部配置，并向客户端报告新的主节点地址，这样客户端就可以自动重新连接到新的主节点，无需人工干预。

**为什么需要多个哨兵节点？**

Redis Sentinel 本身也是一个分布式系统，通常建议部署至少三个哨兵实例。这是为了避免“单点故障”和“脑裂”问题：

- **避免误判：** 单个哨兵可能会因为网络问题或自身故障而误判主节点下线。多个哨兵通过投票机制（Quorum）来达成共识，可以大大降低误判的风险。
- **提高健壮性：** 如果一个或几个哨兵节点发生故障，只要还有足够多的哨兵节点正常运行，整个哨兵系统仍然可以正常工作。

总之，Redis 哨兵机制是实现 Redis 高可用性的核心组件，它通过自动化监控、故障检测和故障转移，大大简化了 Redis 集群的运维，并确保了即使在主节点发生故障时也能提供不间断的服务。

### 2. 为什么要有哨兵机制

在 Redis 的主从架构中，由于主从模式是读写分离的，如果主节点（master）挂了，那么将没有主节点来服务客户端的写操作请求，也没有主节点给从节点（slave）进行数据同步了。

这时如果要恢复服务的话，需要人工介入，选择一个「从节点」切换为「主节点」，然后让其他从节点指向新的主节点，同时还需要通知上游那些连接 Redis 主节点的客户端，将其配置中的主节点 IP 地址更新为「新主节点」的 IP 地址。

这样也不太“智能”了，要是有一个节点能监控「主节点」的状态，当发现主节点挂了 ，它自动将一个「从节点」切换为「主节点」的话，那么可以节省我们很多事情啊！

Redis 在 2.8 版本以后提供的**哨兵（\*Sentinel\*）机制**，它的作用是实现**主从节点故障转移**。它会监测主节点是否存活，如果发现主节点挂了，它就会选举一个从节点切换为主节点，并且把新主节点的相关信息通知给从节点和客户端。

### 3. 哨兵机制是如何工作的

哨兵其实是一个运行在特殊模式下的 **Redis 进程**，所以它也是一个节点。从“哨兵”这个名字也可以看得出来，它相当于是 **“观察者节点”**，观察的对象是 **主从节点**。

当然，它不仅仅是观察那么简单，在它观察到有异常的状况下，会做出一些“动作”，来修复异常状态。

哨兵节点主要负责三件事情：<font color=blue>**监控、选主、通知**</font>。

所以，我们重点要学习这三件事情：

- 哨兵节点是如何监控节点的？又是如何判断主节点是否真的故障了？
- 根据什么规则选择一个从节点切换为主节点？
- 怎么把新主节点的相关信息通知给从节点和客户端呢？

### 4. 如何判断主节点真的故障了

哨兵会每隔 1 秒给所有主从节点发送 PING 命令，当主从节点收到 PING 命令后，会发送一个响应命令给哨兵，这样就可以判断它们是否在正常运行。

如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「**主观下线**」。这个「规定的时间」是配置项  `down-after-milliseconds` 参数设定的，单位是毫秒。

客观下线只适用于主节点。

之所以针对「主节点」设计「主观下线」和「客观下线」两个状态，是因为有可能「主节点」其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令。

所以，为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成**哨兵集群**（*最好是奇数，最少需要三台机器来部署哨兵集群*），**通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况**。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。

当一个哨兵判断主节点为「主观下线」后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应。

当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」。

例如，现在有 3 个哨兵，quorum 配置的是 2，那么一个哨兵需要 2 张赞成票，就可以标记主节点为“客观下线”了。这 2 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。

> quorum 的值一般设置为哨兵个数的二分之一加1，例如 3 个哨兵就设置 2。

哨兵判断完主节点客观下线后，哨兵就要开始在多个「从节点」中，选出一个从节点来做新主节点。

### 5. 由哪个哨兵进行故障转移

前面说过，为了更加“客观”的判断主节点故障了，一般不会只由单个哨兵的检测结果来判断，而是多个哨兵一起判断，这样可以减少误判概率，所以**哨兵是以哨兵集群的方式存在的**。

问题来了，由哨兵集群中的哪个节点进行主从故障转移呢？

所以这时候，还需要在哨兵集群中选出一个 leeder，让 leeder 来执行主从切换。

选举 leeder 的过程其实是一个投票的过程，在投票开始前，肯定得有个「候选者」。

> 那谁来作为候选者呢？

哪个哨兵节点判断主节点为「客观下线」，这个哨兵节点就是候选者，所谓的候选者就是想当 Leader 的哨兵。

举个例子，假设有三个哨兵。当哨兵 B 先判断到主节点「主观下线后」，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他哨兵会根据自己和主节点的网络连接情况，做出赞成投票或者拒绝投票的响应。

当哨兵 B 收到赞成票数达到哨兵配置文件中的 quorum 配置项设定的值后，就会将主节点标记为「客观下线」，此时的哨兵 B 就是一个Leader 候选者。

> 候选者如何选举成为 Leader？

候选者会向其他哨兵发送命令，表明希望成为 Leader 来执行主从切换，并让所有其他哨兵对它进行投票。

每个哨兵只有一次投票机会，如果用完后就不能参与投票了，可以投给自己或投给别人，但是只有候选者才能把票投给自己。

那么在投票过程中，任何一个「候选者」，要满足两个条件：

- 第一，拿到半数以上的赞成票；
- 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

> 注意，对于第一条，得到半数以上的赞成票，是不考虑某些哨兵掉线的情况的。例如，有 5 个哨兵，但是 2 个哨兵掉线，那么依然需要 (5+1)/2=3 张赞成票。

举个例子，假设哨兵节点有  3 个，quorum 设置为 2，那么任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以选举成功了。如果没有满足条件，就需要重新进行选举。

这时候有的同学就会问了，如果某个时间点，刚好有两个哨兵节点判断到主节点为客观下线，那这时不就有两个候选者了？这时该如何决定谁是 Leader 呢？

每位候选者都会先给自己投一票，然后向其他哨兵发起投票请求。如果投票者先收到「候选者 A」的投票请求，就会先投票给它，如果投票者用完投票机会后，收到「候选者 B」的投票请求后，就会拒绝投票。这时，候选者 A 先满足了上面的那两个条件，所以「候选者 A」就会被选举为 Leader。

> 为什么哨兵节点至少要有 3 个？

如果哨兵集群中只有 2 个哨兵节点，此时如果一个哨兵想要成功成为 Leader，必须获得 2 票，而不是 1 票。

所以，如果哨兵集群中有个哨兵挂掉了，那么就只剩一个哨兵了，如果这个哨兵想要成为 Leader，这时票数就没办法达到 2 票，就无法成功成为 Leader，这时是无法进行主从节点切换的。

因此，通常我们至少会配置 3 个哨兵节点。这时，如果哨兵集群中有个哨兵挂掉了，那么还剩下两个个哨兵，如果这个哨兵想要成为 Leader，这时还是有机会达到 2 票的，所以还是可以选举成功的，不会导致无法进行主从节点切换。

当然，你要问，如果 3 个哨兵节点，挂了 2 个怎么办？这个时候得人为介入了，或者增加多一点哨兵节点。

- **哨兵集群可以判定主节点“客观下线”**。哨兵集群还剩下 3 个哨兵，当一个哨兵判断主节点“主观下线”后，询问另外 2 个哨兵后，有可能能拿到 3 张赞同票，这时就达到了 quorum 的值，因此，哨兵集群可以判定主节点为“客观下线”。
- **哨兵集群可以完成主从切换**。当有个哨兵标记主节点为「客观下线」后，就会进行选举 Leader 的过程，因为此时哨兵集群还剩下 3 个哨兵，那么还是可以拿到半数以上（5/2+1=3）的票，而且也达到了 quorum 值，满足了选举 Leader 的两个条件， 所以就能选举成功，因此哨兵集群可以完成主从切换。

如果 quorum 设置为 2 ，并且如果有 3 个哨兵故障的话。此时哨兵集群还是可以判定主节点为“客观下线”，但是哨兵不能完成主从切换了，大家可以自己推演下。

如果 quorum 设置为 3，并且如果有 3 个哨兵故障的话，哨兵集群即不能判定主节点为“客观下线”，也不能完成主从切换了。

可以看到，quorum 为 2 的时候，并且如果有 3 个哨兵故障的话，虽然可以判定主节点为“客观下线”，但是不能完成主从切换，这样感觉「判定主节点为客观下线」这件事情白做了一样，既然这样，还不如不要做，quorum 为 3 的时候，就可以避免这种无用功。

所以，**quorum 的值建议设置为哨兵个数的二分之一加1**，例如 3 个哨兵就设置 2，5 个哨兵设置为 3，而且**哨兵节点的数量应该是奇数**。

### 6. 主从转移的过程是怎样的

在哨兵集群中通过投票的方式，选举出了哨兵 leader 后，就可以进行主从故障转移的过程了。

主从故障转移操作包含以下四个步骤：

#### 6.1 第一步：选出新主节点

在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点。

那么多「从节点」，到底选择哪个从节点作为新主节点的？

随机的方式好吗？随机的方式，实现起来很简单，但是如果选到一个网络状态不好的从节点作为新主节点，那么可能在将来不久又要做一次主从故障迁移。

所以，我们首先要把网络状态不好的从节点给过滤掉。首先把已经下线的从节点过滤掉，然后把以往网络连接状态不好的从节点也给过滤掉。

至此，我们就把网络状态不好的从节点过滤掉了，接下来要对所有从节点进行三轮考察：**优先级、复制进度、ID 号**。在进行每一轮考察的时候，哪个从节点优先胜出，就选择其作为新主节点。

- 第一轮考察：哨兵首先会根据从节点的优先级来进行排序，优先级越小排名越靠前，
- 第二轮考察：如果优先级相同，则查看复制的下标，哪个从「主节点」接收的复制数据多，哪个就靠前。
- 第三轮考察：如果优先级和下标都相同，就选择从节点 ID 较小的那个。

在选举出从节点后，哨兵 leader  向被选中的从节点发送 `SLAVEOF no one` 命令，让这个从节点解除从节点的身份，将其变为新主节点。

在发送 `SLAVEOF no one` 命令之后，哨兵 leader 会以每秒一次的频率向被升级的从节点发送 `INFO` 命令（没进行故障转移之前，`INFO` 命令的频率是每十秒一次），并观察命令回复中的角色信息，当被升级节点的角色信息从原来的 slave 变为 master 时，哨兵 leader 就知道被选中的从节点已经顺利升级为主节点了。

#### 6.2 第二部：将从节点指向新主节点

当新主节点出现之后，哨兵 leader  下一步要做的就是，让已下线主节点属下的所有「从节点」指向「新主节点」，这一动作可以通过向「从节点」发送 `SLAVEOF` 命令来实现。

#### 6.3 第三步：通知客户的主节点已更换

经过前面一系列的操作后，哨兵集群终于完成主从切换的工作，那么新主节点的信息要如何通知给客户端呢？

这主要**通过 Redis 的发布者/订阅者机制来实现**的。每个哨兵节点提供发布者/订阅者机制，客户端可以从哨兵订阅消息。

哨兵提供的消息订阅频道有很多，不同频道包含了主从节点切换过程中的不同关键事件。几个常见的事件如下：

![图片](https://github.com/QaQOwOQaQ/picx-images-hosting/raw/master/640.70albfis6u.webp)

客户端和哨兵建立连接后，客户端会订阅哨兵提供的频道。**主从切换完成后，哨兵就会向  `+switch-master` 频道发布新主节点的 IP 地址和端口的消息，这个时候客户端就可以收到这条信息，然后用这里面的新主节点的 IP 地址和端口进行通信了**。

通过发布者/订阅者机制机制，有了这些事件通知，客户端不仅可以在主从切换后得到新主节点的连接信息，还可以监控到主从节点切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。

#### 6.4 将旧主节点变为从节点

故障转移操作最后要做的是，继续监视旧主节点，当旧主节点重新上线时，哨兵集群就会向它发送 `SLAVEOF` 命令，让它成为新主节点的从节点。

### 7. 哨兵集群是如何工作的

在搭建哨兵集群的时候，只需要填下面几个参数，设置设置主节点名字、主节点的 IP 地址和端口号以及 quorum 值。

```cpp
sentinel monitor <master-name> <ip> <redis-port> <quorum> 
```

而不需要填其他哨兵节点的信息，这是因为 **哨兵节点之间是通过 Redis 的发布者/订阅者机制来相互发现的**。

在主从集群中，主节点上有一个名为`__sentinel__:hello`的频道，不同哨兵就是通过它来相互发现，实现互相通信的。具体的，哨兵节点会将自己的 IP 地址和端口号发布到该频道，这样其它订阅了该频道的哨兵就可以得到该烧饼节点的 IP 地址和端口号，从而与该节点建立网络连接。

哨兵集群会对「从节点」的运行状态进行监控，那哨兵集群如何知道「从节点」的信息？

主节点知道所有「从节点」的信息，所以哨兵会每 10 秒一次的频率向主节点发送 INFO 命令来获取所有「从节点」的信息。具体的，哨兵可以给主节点发送 INFO 命令，主节点接受到这个命令后，就会把从节点列表返回给哨兵。接着，哨兵就可以根据从节点列表中的连接信息，和每个从节点建立连接，并在这个连接上持续地对从节点进行监控。

正是通过  Redis 的发布者/订阅者机制，哨兵之间可以相互感知，然后组成集群，同时，哨兵又通过 INFO 命令，在主节点里获得了所有从节点连接信息，于是就能和从节点建立连接，并进行监控了。

### 8. 总结

Redis 在 2.8 版本以后提供的**哨兵（\*Sentinel\*）机制**，它的作用是实现**主从节点故障转移**。它会监测主节点是否存活，如果发现主节点挂了，它就会选举一个从节点切换为主节点，并且把新主节点的相关信息通知给从节点和客户端。

哨兵一般是以集群的方式部署，至少需要 3 个哨兵节点，哨兵集群主要负责三件事情：**监控、选主、通知**。

哨兵节点通过 Redis 的发布者/订阅者机制，哨兵之间可以相互感知，相互连接，然后组成哨兵集群，同时哨兵又通过 INFO 命令，在主节点里获得了所有从节点连接信息，于是就能和从节点建立连接，并进行监控了。

*1、第一轮投票：判断主节点下线*

当哨兵集群中的某个哨兵判定主节点下线（主观下线）后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应。

当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」。

*2、第二轮投票：选出哨兵 leader*

某个哨兵判定主节点客观下线后，该哨兵就会发起投票，告诉其他哨兵，它想成为 leader，想成为 leader 的哨兵节点，要满足两个条件：

- 第一，拿到半数以上的赞成票；
- 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

*3、由哨兵 leader 进行主从故障转移*

选举出了哨兵 leader 后，就可以进行主从故障转移的过程了。该操作包含以下四个步骤：

- 第一步：在已下线主节点（旧主节点）属下的所有「从节点」里面，挑选出一个从节点，并将其转换为主节点，选择的规则：

- - 过滤掉已经离线的从节点；
  - 过滤掉历史网络连接状态不好的从节点；
  - 将剩下的从节点，进行三轮考察：优先级、复制进度、ID 号。在每一轮考察过程中，如果找到了一个胜出的从节点，就将其作为新主节点。

- 第二步：让已下线主节点属下的所有「从节点」修改复制目标，修改为复制「新主节点」；

- 第三步：将新主节点的 IP 地址和信息，通过「发布者/订阅者机制」通知给客户端；

- 第四步：继续监视旧主节点，当这个旧主节点重新上线时，将它设置为新主节点的从节点；

## 十二、Redis 大 Key

> reference：
>
> * [面试官：Redis 大 key 要如何处理？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247518433&idx=2&sn=e78f630c07f4e60fb78999eb3d742e9e&chksm=f98dcc4bcefa455d8ffde9ad6c8da9b3371a401766a55cbee7af11c87be070d823c8d5926aef&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)
> * [字节二面：Redis 的大 Key 对持久化有什么影响？](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247520683&idx=1&sn=f9e51d382602338d5778dd77daa7f88f&chksm=f98dd501cefa5c1769df4324e8b3cdb880c4bb8aff09ca1e4d444997dc40d585cb431ea5f2d5&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

### 1. 什么是大 Key

大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。

一般而言，下面这两种情况被称为大 key：

- String 类型的值大于 10 KB；
- Hash、List、Set、ZSet 类型的元素的个数超过 5000个；

### 2. 大 key 会造成什么影响

大 key 主要会带来以下四种影响：

- **客户端超时阻塞**。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。
- **引发网络阻塞**。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。
- **阻塞工作线程**。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。
- **内存分布不均**。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。

### 3. 如何找到大 Key

#### 3.1 redis-cli --bigkeys 查找大key

可以通过 redis-cli --bigkeys 命令查找大 key：

```
redis-cli -h 127.0.0.1 -p6379 -a "password" -- bigkeys
```

使用的时候注意事项：

- 最好选择在从节点上执行该命令。因为主节点上执行时，会阻塞主节点；
- 如果没有从节点，那么可以选择在 Redis 实例业务压力的低峰阶段进行扫描查询，以免影响到实例的正常运行；或者可以使用 -i 参数控制扫描间隔，避免长时间扫描降低 Redis 实例的性能。

该方式的不足之处：

- 这个方法只能返回 **每种类型中最大的那个 bigkey**，无法得到大小排在前 N 位的 bigkey；
- **对于集合类型来说，这个方法只统计集合元素个数的多少，而不是实际占用的内存量。**但是，一个集合中的元素个数多，并不一定占用的内存就多。因为，有可能每个元素占用的内存很小，这样的话，即使元素个数有很多，总内存开销也不大；

#### 3.2 使用 SCAN 命令查找大 key

使用 SCAN 命令对数据库扫描，然后用 TYPE 命令获取返回的每一个 key 的类型。

对于 String 类型，可以直接使用 STRLEN 命令获取字符串的长度，也就是占用的内存空间字节数。

对于集合类型来说，有两种方法可以获得它占用的内存大小：

- 如果能够预先从业务层知道集合元素的平均大小，那么，可以使用下面的命令获取集合元素的个数，然后乘以集合元素的平均大小，这样就能获得集合占用的内存大小了。List 类型：`LLEN` 命令；Hash 类型：`HLEN` 命令；Set 类型：`SCARD` 命令；Sorted Set 类型：`ZCARD` 命令；
- 如果不能提前知道写入集合的元素大小，可以使用 `MEMORY USAGE` 命令（需要 Redis 4.0 及以上版本），查询一个键值对占用的内存空间。

> 需要注意的是，`MEMORY USAGE` 命令的时间复杂度并不是 O(1) 的。
>
> 由于直接分析整个总体通常是不切实际的（比如总体太大、成本太高、耗时太长或根本无法获取所有数据），所以通过对样本进行研究，然后将从样本中得出的结论**推断到整个总体**。
>
> 它的核心思想是通过“抽样”（Sampling），以局部平均值代替整体平均值来估算集合的大小。因此，这里存在精确性和效率的冲突。
>
> * 抽样少：效率高，准确率低
> * 抽样多：效率低，准确率高

#### 3.3 使用 RdbTools 工具查找大 key

使用 RdbTools 第三方开源工具，可以用来解析 Redis 快照（RDB）文件，找到其中的大 key。

比如，下面这条命令，将大于 10 kb 的  key  输出到一个表格文件。

```
rdb dump.rdb -c memory --bytes 10240 -f redis.csv
```

### **4. Redis 大 Key 对持久化的影响**

Redis 的持久化方式有两种：AOF 日志和 RDB 快照。

所以接下来，针对这两种持久化方式具体分析分析。

#### 4.1 大 key 对 AOF 日志的影响

* AOF 缓冲区：位于内存中（快速）
* AOF：位于磁盘（慢速）

对 AOF 文件的写入和 C++ 中对文件内容的写入思想都是一样的，每次直接写入到文件中（位于磁盘）是很耗时的，所以一般是先写入到一个缓冲区，然后通过某种策略将缓冲区的内容刷新到文件（磁盘）。

Redis 提供了 3 种 AOF 日志写回硬盘的策略，分别是：

- Always，这个单词的意思是「总是」，所以它的意思是每次写操作命令执行完后，同步将 AOF 日志数据写回硬盘；
- Everysec，这个单词的意思是「每秒」，所以它的意思是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，然后每隔一秒将缓冲区里的内容写回到硬盘，这个刷新操作是异步执行的；
- No，意味着不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，也就是每次写操作命令执行完后，先将命令写入到 AOF 文件的内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘。

这三种策略只是在控制 `fsync()` 函数（将缓冲区中的内容刷新到磁盘）的调用时机。

当应用程序向文件写入数据时，内核通常先将数据复制到内核缓冲区中，然后排入队列，然后由内核决定何时写入硬盘。

如果想要应用程序向文件写入数据后，能立马将数据同步到硬盘，就可以调用 fsync() 函数，这样内核就会将内核缓冲区的数据直接写入到硬盘，等到硬盘写操作完成后，该函数才会返回。

- Always 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数；
- Everysec 策略就会创建一个异步任务来执行 fsync() 函数；
- No 策略就是永不执行 fsync() 函数;

> 分别说说这三种策略，在持久化大 Key 的时候，会影响什么？

在使用 Always 策略的时候，主线程在执行完命令后，会把数据写入到 AOF 日志文件，然后会调用  fsync() 函数，将内核缓冲区的数据直接写入到硬盘，等到硬盘写操作完成后，该函数才会返回。

**当使用 Always 策略的时候，如果写入是一个大 Key，主线程在执行 fsync() 函数的时候，阻塞的时间会比较久，因为当写入的数据量很大的时候，数据同步到硬盘这个过程是很耗时的**。

当使用 Everysec 策略的时候，由于是 **异步** 执行 fsync() 函数，所以大 Key 持久化的过程（数据同步磁盘）不会影响主线程。

当使用 No 策略的时候，由于永不执行 fsync() 函数，所以大 Key 持久化的过程不会影响主线程。

#### 4.2 大 key 对 RDB 快照的影响

当 AOF 日志写入了很多的大 Key，AOF 日志文件的大小会很大，那么很快就会触发 **AOF 重写机制**。

AOF 重写机制和 RDB 快照（bgsave 命令）的过程，都会分别通过 `fork()` 函数创建一个子进程来处理任务。

在创建子进程的过程中，操作系统会把父进程的「页表」复制一份给子进程，这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。

这样一来，子进程就共享了父进程的物理内存数据了，这样能够节约物理内存资源，页表对应的页表项的属性会标记该物理内存的权限为**只读**。

随着 Redis 存在越来越多的大 Key，那么 Redis 就会占用很多内存，对应的页表就会越大。

在通过  `fork()` 函数创建子进程的时候，虽然不会复制父进程的物理内存，但是**内核会把父进程的页表复制一份给子进程，如果页表很大，那么这个复制过程是会很耗时的，那么在执行 fork 函数的时候就会发生阻塞现象**。

而且，fork 函数是由 Redis 主线程调用的，如果 fork 函数发生阻塞，那么意味着就会阻塞 Redis 主线程。由于 Redis 执行命令是在主线程处理的，所以当 Redis 主线程发生阻塞，就无法处理后续客户端发来的命令。

我们可以执行 `info `命令获取到 latest_fork_usec 指标，表示 Redis 最近一次 fork 操作耗时。

```
# 最近一次 fork 操作耗时
latest_fork_usec:315
```

如果 fork 耗时很大，比如超过1秒，则需要做出优化调整：

- 单个实例的内存占用控制在 10 GB 以下，这样 fork 函数就能很快返回。
- 如果 Redis 只是当作纯缓存使用，不关心 Redis 数据安全性问题，可以考虑关闭 AOF 和 AOF 重写，这样就不会调用 fork 函数了。
- 在主从架构中，要适当调大 repl-backlog-size，避免因为  repl_backlog_buffer 不够大，导致主节点频繁地使用全量同步的方式，全量同步的时候，是会创建 RDB 文件的，也就是会调用 fork 函数。

如果创建完子进程后，**父进程对共享内存中的大 Key 进行了修改，那么内核就会发生写时复制，会把物理内存复制一份，由于大 Key 占用的物理内存是比较大的，那么在复制物理内存这一过程中，也是比较耗时的，于是父进程（主线程）就会发生阻塞**。

所以，有两个阶段会导致阻塞父进程：

- 创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长；
- 创建完子进程后，如果子进程或者父进程修改了共享数据，就会发生写时复制，这期间会拷贝物理内存，如果内存越大，自然阻塞的时间也越长；

这里额外提一下， 如果 **Linux 开启了内存大页，会影响 Redis 的性能的**。

Linux 内核从 2.6.38 开始支持内存大页机制，该机制支持 2MB 大小的内存页分配，而常规的内存页分配是按 4KB 的粒度来执行的。

如果采用了内存大页，那么即使客户端请求只修改 100B 的数据，在发生写时复制后，Redis 也需要拷贝 2MB 的大页。相反，如果是常规内存页机制，只用拷贝 4KB。

两者相比，你可以看到，每次写命令引起的**复制内存页单位放大了 512 倍，会拖慢写操作的执行时间，最终导致 Redis 性能变慢**。

那该怎么办呢？很简单，关闭内存大页（默认是关闭的）。

禁用方法如下：

```shell
echo never >  /sys/kernel/mm/transparent_hugepage/enabled
```

#### 4.3 为什么说复制页表是一个很耗时的操作

首先，如果 Redis 是 10GB，一个页表可以存储 4KB，那么一共需要 2.5M（2500000，2500w） 个页表，这个基数就很大了。

此外，每个 PTE（Page Table Entry） 都包含着重要的映射信息，例如物理页帧号、访问权限位（读/写/执行）、脏位、访问位等。当复制页表时，需要将这些大量的页表项从一个进程的页表结构复制到另一个进程的页表结构中，这会占用大量的内存并导致大量的内存复制操作。

如果页表大小为 4KB，那么每个页表项包含 12bit 的数据，复制 2.5M 个页表项，就需要传输 12*2.5M/8B=3.75MB 的数据。如果每秒有 1000 次访问请求，那么每次就需要拷贝 3750MB 的数据，这些数据需要在内存中进行传输，会消耗大量的 CPU 时间和内存带宽。

#### 4.4 总结

当 AOF 写回策略配置了 Always 策略，如果写入是一个大 Key，主线程在执行 fsync() 函数的时候，阻塞的时间会比较久，因为当写入的数据量很大的时候，数据同步到硬盘这个过程是很耗时的。

AOF 重写机制和 RDB 快照（bgsave 命令）的过程，都会分别通过 `fork()` 函数创建一个子进程来处理任务。会有两个阶段会导致阻塞父进程（主线程）：

- 创建子进程的途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大，阻塞的时间也越长；
- 创建完子进程后，如果父进程修改了共享数据中的大 Key，就会发生写时复制，这期间会拷贝物理内存，由于大 Key 占用的物理内存会很大，那么在复制物理内存这一过程，就会比较耗时，所以有可能会阻塞父进程。

大 key 除了会影响持久化之外，还会有以下的影响。

- 客户端超时阻塞。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。
- 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。
- 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。
- 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。

如何避免大 Key 呢？

最好在设计阶段，就把大 key 拆分成一个一个小 key。或者，定时检查 Redis 是否存在大 key ，如果该大 key 是可以删除的，不要使用 DEL 命令删除，因为该命令删除过程会阻塞主线程，而是用 unlink 命令（Redis 4.0+）删除大 key，因为该命令的删除过程是异步的，不会阻塞主线程。 

### 5. 如何删除大 key

删除操作的本质是要释放键值对占用的内存空间，不要小瞧内存的释放过程。

释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序。

**所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞**，如果主线程发生了阻塞，其他所有请求可能都会超时，超时越来越多，会造成 Redis 连接耗尽，产生各种异常。

因此，删除大 key 这一个动作，我们要小心。具体要怎么做呢？这里给出两种方法：

- 分批次删除
- 异步删除（Redis 4.0版本以上）

#### 5.1 分批次删除

对于**删除大 Hash**，使用 `hscan` 命令，每次获取 100 个字段，再用 `hdel` 命令，每次删除 1 个字段。

Python代码：

```python
def del_large_hash():
  r = redis.StrictRedis(host='redis-host1', port=6379)
    large_hash_key ="xxx" #要删除的大hash键名
    cursor = '0'
    while cursor != 0:
        # 使用 hscan 命令，每次获取 100 个字段
        cursor, data = r.hscan(large_hash_key, cursor=cursor, count=100)
        for item in data.items():
                # 再用 hdel 命令，每次删除1个字段
                r.hdel(large_hash_key, item[0])
```

对于**删除大 List**，通过 `ltrim` 命令，每次删除少量元素。

Python代码：

```python
def del_large_list():
  r = redis.StrictRedis(host='redis-host1', port=6379)
  large_list_key = 'xxx'  #要删除的大list的键名
  while r.llen(large_list_key)>0:
      #每次只删除最右100个元素
      r.ltrim(large_list_key, 0, -101) 
```

对于**删除大 Set**，使用 `sscan` 命令，每次扫描集合中 100 个元素，再用 `srem` 命令每次删除一个键。

Python代码：

```python
def del_large_set():
  r = redis.StrictRedis(host='redis-host1', port=6379)
  large_set_key = 'xxx'   # 要删除的大set的键名
  cursor = '0'
  while cursor != 0:
    # 使用 sscan 命令，每次扫描集合中 100 个元素
    cursor, data = r.sscan(large_set_key, cursor=cursor, count=100)
    for item in data:
      # 再用 srem 命令每次删除一个键
      r.srem(large_size_key, item)
```

对于**删除大 ZSet**，使用 `zremrangebyrank` 命令，每次删除 top 100个元素。

Python代码：

```python
def del_large_sortedset():
  r = redis.StrictRedis(host='large_sortedset_key', port=6379)
  large_sortedset_key='xxx'
  while r.zcard(large_sortedset_key)>0:
    # 使用 zremrangebyrank 命令，每次删除 top 100个元素
    r.zremrangebyrank(large_sortedset_key,0,99) 
```

### **2、异步删除**

从 Redis 4.0 版本开始，可以采用**异步删除**法，**用 unlink 命令代替 del 来删除**。

这样 Redis 会将这个 key 放入到一个异步线程中进行删除，这样不会阻塞主线程。

## 十三、Redis 为什么使用跳表，而不用平衡树

为什么 Redis 的 ZSet 对象的底层数据结构之一是跳表而不是平衡树（如红黑树、AVL 树）？

对于这个问题，Redis的作者 @antirez 是怎么说的：

> There are a few reasons:
>
> 1. They are not very ***memory intensive***. It's up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees.
> 2. A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the ***cache locality*** of skip lists is at least as good as with other kind of balanced trees.
> 3. They are ***simpler*** to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code.

简单翻译一下，主要是从内存占用、对范围查找的支持、实现难易程度这三方面总结的原因：

- 它们不是非常内存密集型的。基本上由你决定。改变关于节点具有给定级别数的概率的参数将使其比 btree 占用更少的内存。
- Zset 经常需要执行 ZRANGE 或 ZREVRANGE 的命令，即作为链表遍历跳表。通过此操作，跳表的缓存局部性至少与其他类型的平衡树一样好。
- 它们更易于实现、调试等。例如，由于跳表的简单性，我收到了一个补丁（已经在Redis master中），其中扩展了跳表，在 O(log(N) 中实现了 ZRANK。它只需要对代码进行少量修改。

我再详细补充点：

- **从内存占用上来比较，跳表比平衡树更灵活一些**。平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。
- **在做范围查找的时候，跳表比平衡树操作要简单**。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。
- **从算法实现难度上来比较，跳表比平衡树要简单得多**。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。

## 十四、Redis 9 种数据结构

> reference:
>
> * [2 万字 + 20张图｜ 细说 Redis 九种数据类型和应用场景](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247514054&idx=2&sn=ad92fe82c3d468b97501b84d7b2f8d39&chksm=f98df36ccefa7a7ae8c22fcca7f8d5b857463fb48708b2ae90ba1856271a0bae91c975687f92&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)
> * [为了拿捏 Redis 数据结构，我画了 40 张图（完整版）](https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&mid=2247501112&idx=1&sn=e42b6c61c6747e2c2f3b890ab4e4b844&chksm=f98d8192cefa0884606c5284499d76eeb3966ac2d3de9fbc4a405448313dcf79eb41b7c9501e&scene=178&cur_album_id=1790401816640225283&search_click_id=#rd)

























